{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280ca20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a10114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4443131a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/unlea/Desktop/Python 3/ML_Master/python-machine-learning-book-3rd-edition/ch08')\n",
    "df = pd.read_csv('movie_data.csv',encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042cdf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70eba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "#文章の配列を、BoW表現にする\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer()\n",
    "\n",
    "#カウントベースのvectorizerに渡すとき、空白などで区切られた単語の配列になっている必要がある\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and The weather is sweet'\n",
    "    \n",
    "])\n",
    "#fit_transformメソッドで、3つの文章を特徴量ベクトルに変換している\n",
    "bag =CV.fit_transform(docs)\n",
    "\n",
    "#特徴量となった単語たち(値は単語ID)\n",
    "print(CV.vocabulary_)\n",
    "\n",
    "#祖ベクトルを密なベクトルに変換\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21122303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43370786 0.55847784 0.55847784 0.         0.43370786\n",
      "  0.        ]\n",
      " [0.         0.43370786 0.         0.         0.55847784 0.43370786\n",
      "  0.55847784]\n",
      " [0.40474829 0.47810172 0.30782151 0.30782151 0.30782151 0.47810172\n",
      "  0.30782151]]\n"
     ]
    }
   ],
   "source": [
    "#TFIDFベクトル化をおこなう\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "TFIDF = TfidfTransformer(use_idf=True,#idf価使う\n",
    "                         norm='l2',#得られたTFIDF価からなる行列を、L2ノルムで正規化する\n",
    "                         #smooth_idf=True#TFIDF価の計算のとき,IDF = log((1+nd)/(1+td)) + 1←これをつけるかどうか\n",
    "                         #最も頻出で全文に出てくる単語はidf=0になるが、1をつけることでTFIDFとの掛けの際に無視しないことができる\n",
    "                        )\n",
    "\n",
    "tfidfvec = TFIDF.fit_transform(bag)\n",
    "\n",
    "#TTFIDF行列を密行列にして表示\n",
    "print(tfidfvec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aadbb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#コーパスデータを処理する前に、データのクレンジングが必要\n",
    "df.loc[0,'review'][-50:]#このように、句読点やHTMLコードが入っているので、それは消す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451cf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規表現を使って、消したい文字を消す関数\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    \n",
    "    #アルファベット、アンダーバー、数字以外の文字 \\W\n",
    "    #については、空白置き換えののち全文小文字にする\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f51d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#動作確認\n",
    "preprocessor(df.loc[0,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789c00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#もとのdfのreview本文をすべてクレンジングしておく\n",
    "df['review'] = df['review'].map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4100be2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they',\n",
       " 'can',\n",
       " 'ignor',\n",
       " 'you',\n",
       " 'but',\n",
       " 'no',\n",
       " 'one',\n",
       " 'can',\n",
       " 'be',\n",
       " 'safe',\n",
       " 'from',\n",
       " 'your',\n",
       " 'power']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#空白で分かち書きし、語幹を取り出す（ステミング）\n",
    "#PorterStemmer(語幹を取り出す)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    #リスト内包表記の形で、コーパスを空白区切りして単語のリストにした上で、語幹取り出し、リストにまとめる\n",
    "    return [ps.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer('They can ignore you but no one can be safe from your POWER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23934edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\unlea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72788f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 115, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "TypeError: 'PorterStemmer' object is not callable\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.49894004        nan 0.49894004        nan 0.49894004        nan\n",
      " 0.49894004        nan 0.81388753        nan 0.79408825        nan\n",
      " 0.81892741        nan 0.801608          nan 0.80392807        nan\n",
      " 0.80224813        nan 0.75808976        nan 0.75684981        nan\n",
      " 0.85884577        nan 0.85324592        nan 0.85016617        nan\n",
      " 0.84524609        nan 0.87384498        nan 0.87624509        nan\n",
      " 0.86852542        nan 0.87120537        nan 0.88692453        nan\n",
      " 0.88736446        nan 0.8850448         nan 0.89024438        nan\n",
      " 0.87204512        nan 0.87456505        nan 0.88876447        nan\n",
      " 0.89616425        nan 0.88640461        nan 0.89200438        nan\n",
      " 0.89932406        nan 0.90744379        nan 0.8633655         nan\n",
      " 0.86544537        nan 0.88472471        nan 0.89384447        nan\n",
      " 0.87616501        nan 0.88136482        nan 0.90196383        nan\n",
      " 0.91088363        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x0000024CDE7F75E0>} \n",
      "CV Accuracy: 0.911\n"
     ]
    }
   ],
   "source": [
    "#コーパスをベクトル化して、ロジ回帰につっこみ、感情判定を行う\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1),(1, 2)],#unigram,,bi-gram\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, ps],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [0.01,0.1,1.0, 10.0, 100.0]},\n",
    "\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2150ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizerと、csvから本文とスコアの取り出し関数を定義する\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>','',text)#糞HTMLコードを削除する\n",
    "    #findall(pattern,str)でstrからpatternにマッチする要素をリストの形で取得する\n",
    "    #emotions = re.findall('(?::|;|=)(?:-)(?:|)(?:\\)',text.lower())\n",
    "    text = re.sub('[/W]+',' ',text.lower())# + ' '.join(emotions).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        next(f)#各ループの先頭の要素を取得するが、forloopの外なので、先頭の要素(=header)を読み飛ばす処理をしている\n",
    "        for line in f:\n",
    "            \n",
    "            #print(line)\n",
    "            text,label = line[:-3],int(line[-2])\n",
    "            yield text,label\n",
    "            #print(text,label)\n",
    "\n",
    "next(stream_docs(path='C:/Users/unlea/Desktop/Python 3/ML_Master/python-machine-learning-book-3rd-edition/ch08/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07cbf943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatch関数を定義する\n",
    "def minibatch(doc_stream,size):\n",
    "    docs, y  = [], []\n",
    "    \n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text,label = next(doc_stream)#forループの中で、毎回のループの先頭の要素を取得する\n",
    "            \n",
    "            docs.append(text)#本文をリストに格納\n",
    "            y.append(label)#スコアをリストに格納する\n",
    "            \n",
    "    except StopIteration:\n",
    "        print('end')\n",
    "        return None,None\n",
    "        \n",
    "    return docs,y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb903485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#コーパスのすべてを一度にメモリに乗せるのではなく、ミニバッチ学習で逐次パラメーター更新をおこなうことで計算負荷を軽くする\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#hashing vectorizer\n",
    "vec = HashingVectorizer(decode_error='ignore',\n",
    "                        n_features=2**21,\n",
    "                        preprocessor=None,\n",
    "                        tokenizer=tokenizer)#オレオレtokenizer\n",
    "\n",
    "clf = SGDClassifier(loss='log',random_state=0)\n",
    "\n",
    "doc_stream = stream_docs(path='C:/Users/unlea/Desktop/Python 3/ML_Master/python-machine-learning-book-3rd-edition/ch08/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d647675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.8558\n"
     ]
    }
   ],
   "source": [
    "#ミニバッチ手法を活用したアウトオブコア学習\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "\n",
    "for _ in range(45):#４５回のループ\n",
    "    \n",
    "    X_train,y_train = minibatch(doc_stream,size=1000)#データ数1000のミニバッチで、45batch,45000個のデータをtrainとして順次ミニバッチ学習\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vec.transform(X_train)\n",
    "    \n",
    "    clf.partial_fit(X_train,y_train,classes=classes)#partial_fitでデータを逐次fitさせる\n",
    "    pbar.update()\n",
    "    \n",
    "X_test,y_test = minibatch(doc_stream,size=5000)#最後の5000個のデータで精度評価する\n",
    "X_test = vec.transform(X_test)\n",
    "\n",
    "#testデータでの精度\n",
    "print('score:{}'.format(clf.score(X_test,y_test)))\n",
    "#ミニバッチ学習だと若干精度は下がるけれども、すべてのコーパスをメモリに乗せた状態でベクトル化して識別機を訓練するよりは\n",
    "#計算コストが圧倒的に低い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a11b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5000)\n"
     ]
    }
   ],
   "source": [
    "#潜在ディリクレ分布(LDA,トピックモデル)について学ぶ\n",
    "#LDAは入力として、BoW行列を作成する\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#CV\n",
    "Count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,#単語の最大出現頻度。全文書の１０％以上の文書に出現している単語は、頻出単語なので価値がなく、ゆえに除外する\n",
    "                        max_features=5000)#BoW行列の特徴量の数を5000個に絞る\n",
    "\n",
    "#TFIDF\n",
    "tfidf = TfidfVectorizer(lowercase=True, preprocessor=None, tokenizer=tokenizer, \n",
    "                        ngram_range=(1,2),stop_words=stop,max_df=.1,max_features=5000)\n",
    "#Bowにする\n",
    "#X = Count.fit_transform(df['review'].values)\n",
    "X = tfidf.fit_transform(df['review'].values)#TFIDF vectorization\n",
    "\n",
    "#LDA推定器をBoW行列にFitさせる\n",
    "from sklearn.decomposition import LatentDirichletAllocation#LDA\n",
    "lda = LatentDirichletAllocation(n_components=8,#ハイパーパラメタとして与えるトピックの数\n",
    "                                random_state=0,\n",
    "                                learning_method='batch')#'online'はいわゆるミニバッチ学習。'batch'とすると、毎回のイテレーションで\n",
    "                                #BoW行列をすべて利用し、パラメーターの更新を行う。\n",
    "    \n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "#lda推定器のcomponents_属性にアクセスすると、トピック数ごとに、単語(5000words)の重要度を含んだ行列を返してくれる\n",
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd3d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "book family read read book gay\n",
      "2\n",
      "excellent wonderful comedy music recommend\n",
      "3\n",
      "role performance woman family plays\n",
      "4\n",
      "horror minutes gore boring killer\n",
      "5\n",
      "action fi sci sci fi effects\n",
      "6\n",
      "series episode tv kids watched\n",
      "7\n",
      "worst waste awful terrible money\n",
      "8\n",
      "war documentary series american history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unlea\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#10個のトピックを指定したが、n個のトピック(に分類されるとLDAが判定した文書セット)の、最も重要度の高い単語TOP5がなにか？をみてみる\n",
    "n_top = 5\n",
    "\n",
    "#feature_names = CV.get_feature_names()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for topic_idx,topic in enumerate(lda.components_):\n",
    "    print(topic_idx + 1)\n",
    "    #topicは各10トピックに分類した5000単語の重要度を格納したベクトル\n",
    "    #topic.argsort()で、昇順に並べ替えたインデックスの配列を取得する\n",
    "    #重要度順において昇順になっているので、argsort()[::-1]で降順にしたうえで最初の5つのindexを取得する\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[::-1][:n_top]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7212d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horror movie: 1\n",
      "already his first claim that desires are always artificial is totally fallacious when a jehovah witness reject gets his own documentary on movies or anything for that matter it s time for anyone to get their own although far far more intelligent than say paris hilton i know not too difficult zizek s\n",
      "horror movie: 2\n",
      "in a quiet town a couple of girls witness the murder of one of their friends to a strange young boy named milo who lives on the other side of town after the murder his body is found in a river and his pronounced dead so sixteen years later a weddings draws the girls back to their childhood town and \n",
      "horror movie: 3\n",
      "while filming an 80 s horror movie called hot blooded the director is brutally murdered and the leading lady is scarred as she survives the attack and manages to kill murderer after all of this the production is abandoned and the stock reels are left to gather dust so a group of filmmakers decide to\n"
     ]
    }
   ],
   "source": [
    "#各topic groupごとの重要度TOP5の単語を見ていると、例えばtopic4はホラー映画っぽい。。。\n",
    "#というわけで、トピック4に分類されると判定されたオリジナルのレビューセットをみてみる\n",
    "\n",
    "#X_topicsの各列が、各トピックグループにおける、全レビューの「そのトピックに属してる度」みたいなもので\n",
    "#argsort[::-1]で、度のトピックに属してる度が高いレビューのindexを取り出す\n",
    "horror = X_topics[:,3].argsort()[::-1]\n",
    "\n",
    "#horrorトピックに入っていると判定された上位3レビューについて\n",
    "for idx,midx in enumerate(horror[:3]):\n",
    "    print('horror movie: {}'.format(idx+1))\n",
    "    #先頭の300文字出力\n",
    "    print(df['review'][midx][:300])#確かにホラー映画のれびゅーっぽい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a888d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'already his first claim that desires are always artificial is totally fallacious when a jehovah witness reject gets his own documentary on movies or anything for that matter it s time for anyone to get their own although far far more intelligent than say paris hilton i know not too difficult zizek s mouth spews just as much baloney as hers just a different kind he combines the worst from both his professional worlds psychoanalysis and philosophy both fields are notorious for conveniently offering the expert b lls osopher plenty of leeway to create unprovable theories to rant without a beginning or end and to connect concepts almost randomly in the process misusing the english language by creating a semantic jumble only a mother can love example there are three main marx brothers hence what a great idea to connect them with three levels of human consciousness the id the ego and the super ego i m kind of surprised he didn t play a clip from snowhite and make an analogy between the seven dwarfs and the seven levels of gahannah moslem hell it s like the premise of schumacher s the number 23 play with numbers long enough and you can come up with any kind of cockamamie theory you want even linking ancient greeks with princess di s death however there is an entertainment element to tpgtc watching a raving lunatic sweat like a hog while uttering delusional chants masked as intellectual analysis can be quite a lot of fun why watch cuckoo s nest or any other madhouse drama when you can have zizek for more than 2 hours it s like watching an amusing train wreck admittedly he is almost funny on one or two occasions i have always been mystified by people who desperately try to elevate movie making into an exalted intellectual social science giving idiotic movies like birds this much thought hence this much credit probably has its fat creator laughing in his grave the raw truth is that the vast majority of movies have zero intellectual value and the few ones that do have some intelligence don t require a shrink turned philosopher to draw one a map to understand them unless one is a complete idiot zizek sees layers and layers of meaning in the most banal movies hallucinogenic drugs must be rather popular and cheap in slovenia these days when zizek showed the bathtub hole in the psycho shower scene i thought he was going to say something about galactic black holes how they drain the life out of stars just as the bathtub hole sucks in janet leigh s blood or perhaps he could have said how the hole represents leigh s vagina with the blood flowing into it instead of out as in menstruation this representing some kind of clever zizekian irony speaking of which the real irony is that if hitchcock had really put that much thought into every scene and the script his movies wouldn t have been the illogical far fetched crap that they often are the point of these bathtub hole analogies was to show just how easy it is to improvise about hidden deep meanings and when you add zizek s fanciful terminology from philosophy and psychology layering these terms on top of these analogies like wedding cake decorations you get a rambling jumble that can instantly impress the uneducated i e the easily impressionable and the gullible zizek utters a number of unintentionally funny things here one of the most absurd ideas being when he associates anthony perkins s cleaning of the bloodied bathroom with the satisfaction of work of a job well done don t laugh neither hitchcock nor the writer of psycho could have ever even vaguely entertained this notion that perkins might be enjoying a job well done the cleaning of a blood stained toilet while they were conceiving directing that scene talk about putting words into one s dead mouth but in the context of misinterpreting what the director had to say i like zizek s initial thoughts on tarkovsky s terrific solaris but then he has to ruin a rare good impression by dragging in anti feminism and other nonsense into his theory zizek s attitude towards logic is that of a dog toward its plastic bone i just want to play with it all day logic has its rules and is not supposed to be raped at least not publicly by the likes of him he seems to regard logic proof common sense and reason as enemies or mere throwaway toys concepts to be either avoided twisted to fit the end goal or simply annihilated zizek is the lsd tripped hippie and all his favorite movies are his own personal 2001 s the fact that zizek over focuses on two of the most overrated directors and ones whose films often lack intelligence if anything such as hitchcock and lynch only further diminishes his already low credibility i was surprised de palma didn t feature more prominently that s another lame director who writes inept scripts zizek has a field day with lynch s incomprehensible lost highway there are just as many interpretations of that movie as there are people who watched it zizek s comment that the viewer readily accepts von trier s laughable ground breaking physical set up in dogville made me snicker however zizek doesn t only make up stuff as he goes along he also indulges heavily in the bleedin obvious like all social scientists an oxymoron he wraps his very trite observations into articulate if full of spitting and sometimes complex blankets of language after all sociology functions in precisely the same way it makes us believe we are hearing something new when in fact it s what we already all know but told in an eloquent way which fools the more unobservant listener i was half expecting for men in white suits to suddenly appear out of nowhere and strap him up in a loonie suit slavoj zizek soon as a stalker in a kid s park near you http rateyourmusic com list fedor8 150_worst_cases_of_nepotism '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][24031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b96689d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64d6c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9章頭出し\n",
    "#pickleを使い、モデルを保存する\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:/Users/unlea/Desktop/Python 3/ML_Master/models\")\n",
    "#pickleとしてdump[保存]\n",
    "pickle.dump(stop,open(os.path.join(os.getcwd(),'stopwords.pkl'),'wb'),protocol=4)\n",
    "\n",
    "pickle.dump(clf,open(os.path.join(os.getcwd(),'clf.pkl'),'wb'),protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353b52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "os.chdir(\"C:/Users/unlea/Desktop/Python 3/ML_Master/models\")\n",
    "#pickleとしてdump[保存]\n",
    "pickle.dump(vec,open(os.path.join(os.getcwd(),'HashingVectorizer.pkl'),'wb'),protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f77874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存s似たモデルを呼び出し、任意の文章のvectorizationが可能であることを確認する\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join('C:/Users/unlea/Desktop/Python 3/ML_Master/models',\n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2338ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', random_state=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存したclassifier(pkl形式)を読み出す\n",
    "clf = pickle.load(open(os.path.join('C:/Users/unlea/Desktop/Python 3/ML_Master/models','clf.pkl'),'rb'))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7d3903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : positive, Probability : 63.844%\n"
     ]
    }
   ],
   "source": [
    "#上で書いたHashingVectorizerと、読みだしたclassifierを使い、任意の文章のネガポジ判定をする\n",
    "import numpy as np\n",
    "\n",
    "#ネガポジ判定で返される0/1のラベルに対応した辞書\n",
    "label = {0:'negative',1:'positive'}\n",
    "#サンプル文章\n",
    "example = [\"I think this movie will be the masterpiece of romance movies.\"]\n",
    "\n",
    "#サンプル文章をベクトルに変換する\n",
    "X = vect.transform(example)\n",
    "#サンプル文章がネガポジどちらと判定されるか、また判定されたとしてそのラベルに対応する確率はどれくらいか？を出力する\n",
    "print(\"Prediction : {}, Probability : {:.3f}%\".format\n",
    "      #clf.predict(X)で予測結果のラベルの配列が返ってくるので、[0]で取り出して、ラベル辞書からネガポジなのかを取り出し\n",
    "      (label[clf.predict(X)[0]],\n",
    "       #clf.predict_proba(X)で、各ラベルに所属する確率を返すので、np.max()で最も確率の高い(=predictで予測したラベルに所属する確率)値を取り出す\n",
    "       np.max(clf.predict_proba(X)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86dde855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11067085, 0.88932915]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d60ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
